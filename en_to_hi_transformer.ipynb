{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e5473f",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e25f9263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from string import digits\n",
    "from tensorflow.data import Dataset\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 8\n",
    "buffer_size = 2000\n",
    "max_tokens =128\n",
    "\n",
    "def create_dataset():\n",
    "    df = pd.read_csv('./data/hind_encorp.csv',encoding='utf-8')\n",
    "    #df = df[df['source']=='ted']\n",
    "    \n",
    "    df = df.dropna()\n",
    "    # Lowercase all characters\n",
    "    df['english_sentence']=df['english_sentence'].apply(lambda x: x.lower())\n",
    "    df['hindi_sentence']=df['hindi_sentence'].apply(lambda x: x.lower())\n",
    "    # Remove quotes\n",
    "    df['english_sentence']=df['english_sentence'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "    df['hindi_sentence']=df['hindi_sentence'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "    exclude = set(string.punctuation) # Set of all special characters\n",
    "    # Remove all the special characters\n",
    "    df['english_sentence']=df['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "    df['hindi_sentence']=df['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "    # Remove all numbers from text\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    df['english_sentence']=df['english_sentence'].apply(lambda x: x.translate(remove_digits))\n",
    "    df['hindi_sentence']=df['hindi_sentence'].apply(lambda x: x.translate(remove_digits))\n",
    "\n",
    "    df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n",
    "\n",
    "    # Remove extra spaces\n",
    "    df['english_sentence']=df['english_sentence'].apply(lambda x: x.strip())\n",
    "    df['hindi_sentence']=df['hindi_sentence'].apply(lambda x: x.strip())\n",
    "    df['english_sentence']=df['english_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "    df['hindi_sentence']=df['hindi_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "    \n",
    "    dataset = Dataset.from_tensor_slices((df['english_sentence'].values, df['hindi_sentence'].values))\n",
    "    train_split = 0.8\n",
    "    val_split = 0.2\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = int(val_split * len(dataset))\n",
    "        \n",
    "    \n",
    "    train_encorp_samples, val_encorp_samples = dataset.take(train_size), dataset.skip(train_size).take(val_size)\n",
    "    train_encorp_ds = train_encorp_samples.shuffle(buffer_size).batch(batch_size).map(\n",
    "        lambda en, hi: prepare_en_in_batch((en,hi)), tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    val_encorp_ds = val_encorp_samples.shuffle(buffer_size).batch(batch_size).map(\n",
    "        lambda en, hi: prepare_en_in_batch((en,hi)), tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_encorp_ds, val_encorp_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "264be480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocabulary.tokenizer import TransformerTokenizer\n",
    "from vocabulary.gen_vocab import reserved_tokens\n",
    "\n",
    "def get_tokenizer():\n",
    "    en_bert = './vocabulary/encorp_en_to_hi/en_encorp_vocab.txt'\n",
    "    hi_bert = './vocabulary/encorp_en_to_hi/hi_encorp_vocab.txt'\n",
    "    en_tokenizer = TransformerTokenizer(en_bert, res_tokens=reserved_tokens)\n",
    "    hi_tokenizer = TransformerTokenizer(hi_bert, res_tokens=reserved_tokens)\n",
    "    return en_tokenizer, hi_tokenizer\n",
    "\n",
    "def prepare_en_in_batch( inputs):\n",
    "        e_line ,h_line = inputs\n",
    "        en = src_tokenizer.tokenize(e_line)\n",
    "        en = en[:, :max_tokens]\n",
    "        en = en.to_tensor()\n",
    "\n",
    "        hi = target_tokenizer.tokenize(h_line)\n",
    "        hi = hi[:, : (max_tokens + 1)]\n",
    "        hi_inputs = hi[:, :-1].to_tensor()\n",
    "        hi_labels = hi[:, 1:].to_tensor()\n",
    "\n",
    "        return (en, hi_inputs), hi_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4840b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer, target_tokenizer = get_tokenizer()\n",
    "train_encorp_ds, val_encorp_ds = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a0b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e579fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_en_in(train_batches, src_tokenizer, target_tokenizer):\n",
    "    \n",
    "    for (en_line, hi_line), label in train_batches.take(1):\n",
    "        break\n",
    "    #print('en_line\\n', en_line, type(en_line))\n",
    "    #print('hi_line\\n', hi_line, type(hi_line))\n",
    "    #print('label\\n', label, type(label))\n",
    "\n",
    "    en_tokens = src_tokenizer.detokenize(en_line.numpy())\n",
    "    hi_tokens = target_tokenizer.detokenize(hi_line.numpy())\n",
    "\n",
    "    print('pt line\\n', en_tokens.numpy()[0].decode('utf-8'))\n",
    "    #print('pt tokens\\n', src_tokenizer.lookup(en_line.numpy()))\n",
    "    print('en line\\n', hi_tokens.numpy()[0].decode('utf-8'))\n",
    "    #print('en tokens\\n', target_tokenizer.lookup(hi_line.numpy()))\n",
    "    label_tokens = target_tokenizer.detokenize(label.numpy())\n",
    "    print('en label\\n', label_tokens.numpy()[0].decode('utf-8'))\n",
    "    #print('label tokens\\n', target_tokenizer.lookup(label.numpy()))\n",
    "\n",
    "    print(en_line.shape)\n",
    "    print(hi_line.shape)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1af23ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-06 06:43:31.571213: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_19' with dtype int64\n",
      "\t [[{{node Placeholder/_19}}]]\n",
      "2023-07-06 06:43:31.572101: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_15' with dtype int64\n",
      "\t [[{{node Placeholder/_15}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt line\n",
      " what troubled me was that this topdown approach is still around\n",
      "en line\n",
      " मझ यह परशानी थी कि यह ऊपर स नीच की सोच अब भी मौजद ह ।\n",
      "en label\n",
      " मझ यह परशानी थी कि यह ऊपर स नीच की सोच अब भी मौजद ह ।\n",
      "(8, 90)\n",
      "(8, 93)\n",
      "(8, 93)\n"
     ]
    }
   ],
   "source": [
    "test_en_in(train_encorp_ds,src_tokenizer, target_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3c8ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import TransformerTraining\n",
    "\n",
    "training = TransformerTraining(src_tokenizer, \n",
    "                               target_tokenizer, \n",
    "                               num_layers=4, \n",
    "                               d_mode=512, \n",
    "                               dff=2048,\n",
    "                               num_heads=8, \n",
    "                               dropout_rate=0.1, \n",
    "                               num_epochs=10,\n",
    "                               steps_per_epochs=0.1,\n",
    "                               save_freq=5)\n",
    "training.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2008413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-06 06:43:48.836191: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_19' with dtype int64\n",
      "\t [[{{node Placeholder/_19}}]]\n",
      "2023-07-06 06:43:48.837407: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_16' with dtype int64\n",
      "\t [[{{node Placeholder/_16}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-06 06:44:17.873073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-07-06 06:44:18.343453: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-07-06 06:44:18.838525: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fac9401e5e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-06 06:44:18.838644: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-07-06 06:44:18.957754: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-07-06 06:44:19.811729: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 271/2738 [=>............................] - ETA: 18:26 - loss: nan - masked_accuracy: 0.0403"
     ]
    }
   ],
   "source": [
    "training.fit(train_encorp_ds, val_encorp_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe76b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.translator import TransformerTranslator\n",
    "import tensorflow as tf\n",
    "\n",
    "translator = TransformerTranslator((target_tokenizer, src_tokenizer), training.get_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c165604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translation(sentence, tokens, ground_truth):\n",
    "  print(f'{\"Input:\":15s}: {sentence}')\n",
    "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
    "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef983663",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'politicians do not have permission to do what'\n",
    "ground_truth = 'राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह क'\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66a0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Hello How are you'\n",
    "ground_truth = 'नमस्ते, आप कैसे हैं'\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d85022c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
